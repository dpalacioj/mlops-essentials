{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLHHb5pDe-Kr"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, AdamW"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "esto esun ensayo"
      ],
      "metadata": {
        "id": "NqoHIpytfYTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVlEgxVGe-Kt"
      },
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "\n",
        "params = {\n",
        "    \"model_name\": \"distilbert-base-uncased\",\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"batch_size\":16,\n",
        "    \"num_epochs\": 2,\n",
        "    \"dataset_name\": \"ag_news\",\n",
        "    \"task_name\": \"sequence_classification\",\n",
        "    \"log_steps\": 100,\n",
        "    \"max_seq_length\": 128,\n",
        "    \"output_dir\": \"../models/distilbert_ag-news\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W83hcVkze-Ku"
      },
      "source": [
        "Before configure the tracking uri, run the next command in the bash terminal:<br/>\n",
        "`\n",
        "mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 127.0.0.1 --port 5000\n",
        "`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef45bKAye-Kv",
        "outputId": "5530fc68-c544-412b-b4b5-15c99c74b769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando el experimento existente: 'sequence_classification\n"
          ]
        }
      ],
      "source": [
        "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "\n",
        "\n",
        "client = mlflow.MlflowClient()\n",
        "\n",
        "experiment_name = params[\"task_name\"]\n",
        "experiment = client.get_experiment_by_name(experiment_name)\n",
        "\n",
        "if experiment is None:\n",
        "\n",
        "    mlflow.set_experiment(params[\"task_name\"])\n",
        "    print(f\"Experimento '{experiment_name}' creado\")\n",
        "\n",
        "else:\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "    print(f\"Usando el experimento existente: '{experiment_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e4af548fe3334f28bd89cd997ca1c3aa",
            "f30108d370434d85810c626a0813d941",
            "eb4512bb24754301b4afd17e89dd24a2",
            "103e6bf8d3414451b47ba71727ba8e2e"
          ]
        },
        "id": "njU1YK8Ie-Kw",
        "outputId": "cf8cdcf3-06e3-458e-aa2a-9a724083827f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4af548fe3334f28bd89cd997ca1c3aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f30108d370434d85810c626a0813d941",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb4512bb24754301b4afd17e89dd24a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "103e6bf8d3414451b47ba71727ba8e2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with mlflow.start_run(run_name=f\"{params['model_name']}-{params['dataset_name']}\") as run:\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    # Load dataset\n",
        "\n",
        "    dataset = load_dataset(params['dataset_name'])\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(params['model_name'])\n",
        "\n",
        "    # To be sure our input shape fit well, padding and truncation are used\n",
        "\n",
        "    def tokenize(batch):\n",
        "            return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=params['max_seq_length'])\n",
        "\n",
        "    train_dataset = dataset['train'].shuffle().select(range(7_000)).map(tokenize, batched=True)\n",
        "    test_dataset = dataset['test'].shuffle().select(range(1_000)).map(tokenize, batched=True)\n",
        "\n",
        "    train_dataset.to_parquet('../data/train.parquet')\n",
        "\n",
        "    test_dataset.to_parquet('../data/test.parquet')\n",
        "\n",
        "    mlflow.log_artifact('../data/train.parquet', artifact_path='datasets')\n",
        "    mlflow.log_artifact('../data/test.parquet', artifact_path='datasets')\n",
        "\n",
        "    # Set format for Pytorch and create data loaders\n",
        "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "\n",
        "    # Get the labels\n",
        "    labels = dataset['train'].features['label'].names\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4dyY4kve-Kw"
      },
      "source": [
        "Keep in mind that `input_ids` and `attention_mask` are columns for the Hugging Face models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3jfUdQRe-Kx"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokens = tokenizer(\"This is a sample text.\", padding=\"max_length\", truncation=True, max_length=10)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI_E5n98e-Kx"
      },
      "source": [
        "Now, to help speed up training a bit, will also create some data loaders for our data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0pA8nbHe-Kx",
        "outputId": "6d8a9912-10d1-4857-faa7-2d13cd606b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 120000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 7600\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "['text', 'label']\n",
            "\n",
            "\n",
            "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)}\n"
          ]
        }
      ],
      "source": [
        "print(dataset)\n",
        "\n",
        "print('\\n')\n",
        "print(dataset['train'].column_names)\n",
        "print('\\n')\n",
        "\n",
        "print(dataset['train'].features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLHcIMDxe-Ky"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlops_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}